#!/usr/bin/env python3
"""
ONNXãƒ¢ãƒ‡ãƒ«ã®å‹•ç”»ãƒ»ã‚«ãƒ¡ãƒ©æ¨è«–
"""

import argparse
import json
import time
from pathlib import Path

import cv2
import numpy as np
import onnxruntime as ort


class WaniCalibration:
    """ãƒ¯ãƒ‹ä½ç½®ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚¯ãƒ©ã‚¹"""

    def __init__(self, center_x=320, center_y=240, spacing=100, width=80, height=60, config_file="wani_calibration.json"):
        self.center_x = center_x  # ä¸­å¿ƒä½ç½® X
        self.center_y = center_y  # ä¸­å¿ƒä½ç½® Y
        self.spacing = spacing  # ãƒ¯ãƒ‹é–“ã®è·é›¢
        self.width = width  # ãƒ¯ãƒ‹ã®æ¨ªå¹…
        self.height = height  # ãƒ¯ãƒ‹ã®ç¸¦å¹…
        self.step_size = 5  # èª¿æ•´ã‚¹ãƒ†ãƒƒãƒ—ã‚µã‚¤ã‚º
        self.config_file = config_file

        # ã‚«ãƒ¡ãƒ©å¤‰æ›ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.crop_x = 0  # ã‚¯ãƒ­ãƒƒãƒ—é–‹å§‹Xåº§æ¨™
        self.crop_y = 0  # ã‚¯ãƒ­ãƒƒãƒ—é–‹å§‹Yåº§æ¨™
        self.crop_width = 640  # ã‚¯ãƒ­ãƒƒãƒ—å¹…
        self.crop_height = 480  # ã‚¯ãƒ­ãƒƒãƒ—é«˜ã•
        self.rotation_angle = 0  # å›è»¢è§’åº¦ï¼ˆåº¦ï¼‰
        self.crop_step = 10  # ã‚¯ãƒ­ãƒƒãƒ—èª¿æ•´ã‚¹ãƒ†ãƒƒãƒ—
        self.rotation_step = 1  # å›è»¢èª¿æ•´ã‚¹ãƒ†ãƒƒãƒ—

        # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚Œã°èª­ã¿è¾¼ã¿
        self.load_config()

    def get_wani_zones(self):
        """ãƒ¯ãƒ‹ã®5ã¤ã®ã‚¾ãƒ¼ãƒ³ã‚’å–å¾—"""
        zones = []
        for i in range(5):
            # ä¸­å¤®ã‚’åŸºæº–ã«å·¦å³ã«é…ç½® (-2, -1, 0, 1, 2)
            offset_x = (i - 2) * self.spacing
            x1 = self.center_x + offset_x - self.width // 2
            y1 = self.center_y - self.height // 2
            x2 = self.center_x + offset_x + self.width // 2
            y2 = self.center_y + self.height // 2

            zones.append(
                {"id": f"wani_{i + 1:02d}", "bbox": [x1, y1, x2, y2], "center": [self.center_x + offset_x, self.center_y]}
            )

        return zones

    def assign_detection_to_zone(self, detection_bbox):
        """æ¤œå‡ºãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹ã‚’ã‚¾ãƒ¼ãƒ³ã«å‰²ã‚Šå½“ã¦"""
        det_x1, det_y1, det_x2, det_y2 = detection_bbox
        det_center_x = (det_x1 + det_x2) / 2
        det_center_y = (det_y1 + det_y2) / 2

        zones = self.get_wani_zones()
        best_zone = None
        min_distance = float("inf")

        for zone in zones:
            zone_x1, zone_y1, zone_x2, zone_y2 = zone["bbox"]

            # ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹ã®é‡ãªã‚Šã‚’ãƒã‚§ãƒƒã‚¯
            overlap_x = max(0, min(det_x2, zone_x2) - max(det_x1, zone_x1))
            overlap_y = max(0, min(det_y2, zone_y2) - max(det_y1, zone_y1))
            overlap_area = overlap_x * overlap_y

            if overlap_area > 0:
                # é‡ãªã‚ŠãŒã‚ã‚‹å ´åˆã¯ä¸­å¿ƒã‹ã‚‰ã®è·é›¢ã§åˆ¤å®š
                zone_center_x, zone_center_y = zone["center"]
                distance = np.sqrt((det_center_x - zone_center_x) ** 2 + (det_center_y - zone_center_y) ** 2)

                if distance < min_distance:
                    min_distance = distance
                    best_zone = zone["id"]

        return best_zone

    def get_status_text(self):
        """ç¾åœ¨ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ–‡å­—åˆ—ã§å–å¾—"""
        return [
            f"Center: ({self.center_x}, {self.center_y})",
            f"Spacing: {self.spacing}",
            f"Size: {self.width}x{self.height}",
            f"Crop: ({self.crop_x},{self.crop_y}) {self.crop_width}x{self.crop_height}",
            f"Rotation: {self.rotation_angle}Â°",
        ]

    def save_config(self):
        """è¨­å®šã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        config = {
            "center_x": self.center_x,
            "center_y": self.center_y,
            "spacing": self.spacing,
            "width": self.width,
            "height": self.height,
            "crop_x": self.crop_x,
            "crop_y": self.crop_y,
            "crop_width": self.crop_width,
            "crop_height": self.crop_height,
            "rotation_angle": self.rotation_angle,
        }
        try:
            with open(self.config_file, "w") as f:
                json.dump(config, f, indent=2)
            print(f"  è¨­å®šä¿å­˜: {self.config_file}")
        except Exception as e:
            print(f"  è¨­å®šä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

    def load_config(self):
        """è¨­å®šã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿"""
        try:
            if Path(self.config_file).exists():
                with open(self.config_file, "r") as f:
                    config = json.load(f)
                self.center_x = config.get("center_x", self.center_x)
                self.center_y = config.get("center_y", self.center_y)
                self.spacing = config.get("spacing", self.spacing)
                self.width = config.get("width", self.width)
                self.height = config.get("height", self.height)
                self.crop_x = config.get("crop_x", self.crop_x)
                self.crop_y = config.get("crop_y", self.crop_y)
                self.crop_width = config.get("crop_width", self.crop_width)
                self.crop_height = config.get("crop_height", self.crop_height)
                self.rotation_angle = config.get("rotation_angle", self.rotation_angle)
                print(f"  è¨­å®šèª­ã¿è¾¼ã¿: {self.config_file}")
        except Exception as e:
            print(f"  è¨­å®šèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")

    def adjust_parameter(self, param_name, direction):
        """ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’èª¿æ•´"""
        if param_name == "center_x":
            self.center_x += direction * self.step_size
        elif param_name == "center_y":
            self.center_y += direction * self.step_size
        elif param_name == "spacing":
            self.spacing = max(20, self.spacing + direction * self.step_size)
        elif param_name == "width":
            self.width = max(20, self.width + direction * self.step_size)
        elif param_name == "height":
            self.height = max(20, self.height + direction * self.step_size)
        elif param_name == "crop_x":
            self.crop_x = max(0, self.crop_x + direction * self.crop_step)
        elif param_name == "crop_y":
            self.crop_y = max(0, self.crop_y + direction * self.crop_step)
        elif param_name == "crop_width":
            self.crop_width = max(100, self.crop_width + direction * self.crop_step)
        elif param_name == "crop_height":
            self.crop_height = max(100, self.crop_height + direction * self.crop_step)
        elif param_name == "rotation":
            self.rotation_angle = (self.rotation_angle + direction * self.rotation_step) % 360

        # èª¿æ•´å¾Œã«è‡ªå‹•ä¿å­˜
        self.save_config()

    def apply_camera_transform(self, frame):
        """ã‚«ãƒ¡ãƒ©ãƒ•ãƒ¬ãƒ¼ãƒ ã«ã‚¯ãƒ­ãƒƒãƒ—ã¨å›è»¢ã‚’é©ç”¨"""
        if frame is None:
            return frame

        # å›è»¢ã‚’é©ç”¨
        if self.rotation_angle != 0:
            height, width = frame.shape[:2]
            center = (width // 2, height // 2)
            rotation_matrix = cv2.getRotationMatrix2D(center, self.rotation_angle, 1.0)
            frame = cv2.warpAffine(frame, rotation_matrix, (width, height))

        # ã‚¯ãƒ­ãƒƒãƒ—ã‚’é©ç”¨
        height, width = frame.shape[:2]
        x1 = max(0, min(self.crop_x, width - 100))
        y1 = max(0, min(self.crop_y, height - 100))
        x2 = min(width, x1 + self.crop_width)
        y2 = min(height, y1 + self.crop_height)

        cropped = frame[y1:y2, x1:x2]

        # ã‚¯ãƒ­ãƒƒãƒ—å¾Œã®ã‚µã‚¤ã‚ºãŒå…ƒã®è§£åƒåº¦ã¨ç•°ãªã‚‹å ´åˆã€ãƒªã‚µã‚¤ã‚º
        if cropped.shape[0] != height or cropped.shape[1] != width:
            cropped = cv2.resize(cropped, (width, height))

        return cropped


def preprocess_frame(frame, img_size=640):
    """ãƒ•ãƒ¬ãƒ¼ãƒ ã®å‰å‡¦ç†"""
    if frame is None:
        raise ValueError("ãƒ•ãƒ¬ãƒ¼ãƒ ãŒNoneã§ã™")

    img = frame.copy()
    original_shape = img.shape[:2]  # H, W

    # ãƒªã‚µã‚¤ã‚ºï¼ˆã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”ã‚’ä¿æŒï¼‰
    scale = img_size / max(original_shape)
    new_shape = (int(original_shape[1] * scale), int(original_shape[0] * scale))
    img_resized = cv2.resize(img, new_shape)

    # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
    dw = (img_size - new_shape[0]) / 2
    dh = (img_size - new_shape[1]) / 2
    top, bottom = int(np.round(dh - 0.1)), int(np.round(dh + 0.1))
    left, right = int(np.round(dw - 0.1)), int(np.round(dw + 0.1))
    img_padded = cv2.copyMakeBorder(img_resized, top, bottom, left, right, cv2.BORDER_CONSTANT, value=(114, 114, 114))

    # æ­£è¦åŒ–ã¨ãƒãƒ£ãƒ³ãƒãƒ«é †åºå¤‰æ›´
    img_normalized = img_padded.astype(np.float32) / 255.0
    img_transposed = np.transpose(img_normalized, (2, 0, 1))  # HWC -> CHW
    img_batch = np.expand_dims(img_transposed, axis=0)  # Add batch dimension

    return img_batch, img, scale, (left, top)


def postprocess_predictions(outputs, img_shape, scale, padding, conf_threshold=0.5):
    """äºˆæ¸¬çµæœã®å¾Œå‡¦ç†"""
    predictions = outputs[0]  # Shape: (1, 300, 5) - [x, y, w, h, confidence]

    detections = []
    for pred in predictions[0]:  # 300å€‹ã®äºˆæ¸¬
        x, y, w, h, conf = pred

        if conf < conf_threshold:
            continue

        # ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹ã‚’centerå½¢å¼ã‹ã‚‰cornerå½¢å¼ã«å¤‰æ›
        x1 = x - w / 2
        y1 = y - h / 2
        x2 = x + w / 2
        y2 = y + h / 2

        # 640x640åº§æ¨™ã‹ã‚‰å…ƒç”»åƒåº§æ¨™ã«å¤‰æ›
        # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã‚’è€ƒæ…®
        x1 = (x1 * 640 - padding[0]) / scale
        y1 = (y1 * 640 - padding[1]) / scale
        x2 = (x2 * 640 - padding[0]) / scale
        y2 = (y2 * 640 - padding[1]) / scale

        # ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°
        x1 = max(0, min(x1, img_shape[1]))
        y1 = max(0, min(y1, img_shape[0]))
        x2 = max(0, min(x2, img_shape[1]))
        y2 = max(0, min(y2, img_shape[0]))

        detections.append({"bbox": [x1, y1, x2, y2], "confidence": float(conf), "class": "wani"})

    return detections


def draw_calibration_zones(img, calibration):
    """ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚¾ãƒ¼ãƒ³ã‚’æç”»"""
    zones = calibration.get_wani_zones()
    colors = [(255, 0, 0), (0, 255, 0), (0, 0, 255), (255, 255, 0), (255, 0, 255)]  # ã‚¾ãƒ¼ãƒ³ã”ã¨ã®è‰²

    for i, zone in enumerate(zones):
        x1, y1, x2, y2 = [int(coord) for coord in zone["bbox"]]
        color = colors[i % len(colors)]

        # ã‚¾ãƒ¼ãƒ³ã®æ ã‚’æç”»ï¼ˆç‚¹ç·šï¼‰
        cv2.rectangle(img, (x1, y1), (x2, y2), color, 2)

        # ã‚¾ãƒ¼ãƒ³IDã‚’æç”»
        label = zone["id"]
        label_size, _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)
        cv2.rectangle(img, (x1, y1 - label_size[1] - 8), (x1 + label_size[0] + 4, y1), color, -1)
        cv2.putText(img, label, (x1 + 2, y1 - 4), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)

        # ä¸­å¿ƒç‚¹ã‚’æç”»
        center_x, center_y = [int(coord) for coord in zone["center"]]
        cv2.circle(img, (center_x, center_y), 3, color, -1)


def draw_detections_with_zones(img, detections, calibration):
    """æ¤œå‡ºçµæœã¨ã‚¾ãƒ¼ãƒ³ã‚’æç”»"""
    img_draw = img.copy()

    # ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚¾ãƒ¼ãƒ³ã‚’æç”»
    draw_calibration_zones(img_draw, calibration)

    # æ¤œå‡ºçµæœã‚’æç”»
    for det in detections:
        x1, y1, x2, y2 = [int(coord) for coord in det["bbox"]]
        conf = det["confidence"]

        # ã‚¾ãƒ¼ãƒ³å‰²ã‚Šå½“ã¦
        zone_id = calibration.assign_detection_to_zone(det["bbox"])

        # ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹æç”»
        if zone_id:
            color = (0, 255, 0)  # ã‚¾ãƒ¼ãƒ³å†…ã®å ´åˆã¯ç·‘
            label = f"{zone_id}: {conf:.2f}"
        else:
            color = (0, 165, 255)  # ã‚¾ãƒ¼ãƒ³å¤–ã®å ´åˆã¯ã‚ªãƒ¬ãƒ³ã‚¸
            label = f"unknown: {conf:.2f}"

        cv2.rectangle(img_draw, (x1, y1), (x2, y2), color, 2)

        # ãƒ©ãƒ™ãƒ«æç”»
        label_size, _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)
        cv2.rectangle(img_draw, (x1, y2 + 2), (x1 + label_size[0] + 4, y2 + label_size[1] + 6), color, -1)
        cv2.putText(img_draw, label, (x1 + 2, y2 + label_size[1] + 4), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)

    return img_draw


def draw_detections(img, detections):
    """æ¤œå‡ºçµæœã‚’ç”»åƒã«æç”»ï¼ˆå¾“æ¥ç‰ˆï¼‰"""
    img_draw = img.copy()

    for det in detections:
        x1, y1, x2, y2 = [int(coord) for coord in det["bbox"]]
        conf = det["confidence"]

        # ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹æç”»
        cv2.rectangle(img_draw, (x1, y1), (x2, y2), (0, 255, 0), 2)

        # ãƒ©ãƒ™ãƒ«æç”»
        label = f"wani: {conf:.2f}"
        label_size, _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)
        cv2.rectangle(img_draw, (x1, y1 - label_size[1] - 4), (x1 + label_size[0], y1), (0, 255, 0), -1)
        cv2.putText(img_draw, label, (x1, y1 - 2), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1)

    return img_draw


def run_frame_inference(session, frame, input_name, output_names, conf_threshold=0.5):
    """å˜ä¸€ãƒ•ãƒ¬ãƒ¼ãƒ ã§æ¨è«–å®Ÿè¡Œ"""
    # ãƒ•ãƒ¬ãƒ¼ãƒ å‰å‡¦ç†
    img_input, img_original, scale, padding = preprocess_frame(frame)

    # æ¨è«–å®Ÿè¡Œ
    outputs = session.run(output_names, {input_name: img_input})

    # å¾Œå‡¦ç†
    detections = postprocess_predictions(outputs, img_original.shape[:2], scale, padding, conf_threshold)

    return detections, img_original


def run_camera_inference(
    model_path, camera_id=0, conf_threshold=0.5, record_video=False, fps_limit=30, enable_calibration=False, provider="auto"
):
    """USBã‚«ãƒ¡ãƒ©ã§ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ¨è«–å®Ÿè¡Œ"""
    print(f"\nğŸ“¹ USBã‚«ãƒ¡ãƒ©: {camera_id}")

    # ONNXã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆ
    print("ğŸ“¦ ONNXãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ä¸­...")

    # ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼é¸æŠ
    def get_providers(provider_choice):
        available = ort.get_available_providers()

        if provider_choice == "tensorrt":
            if "TensorrtExecutionProvider" not in available:
                print("âš ï¸  TensorRTæœªå¯¾å¿œã€CUDAã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯")
                return ["CUDAExecutionProvider", "CPUExecutionProvider"]
            return ["TensorrtExecutionProvider", "CPUExecutionProvider"]
        elif provider_choice == "cuda":
            if "CUDAExecutionProvider" not in available:
                print("âš ï¸  CUDAæœªå¯¾å¿œã€CPUã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯")
                return ["CPUExecutionProvider"]
            return ["CUDAExecutionProvider", "CPUExecutionProvider"]
        elif provider_choice == "cpu":
            return ["CPUExecutionProvider"]

    providers = get_providers(provider)

    # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚ªãƒ—ã‚·ãƒ§ãƒ³è¨­å®š
    sess_options = ort.SessionOptions()
    sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL

    # ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼å›ºæœ‰ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼ˆãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã¨åŒã˜æ•°å¿…è¦ï¼‰
    provider_options = []

    if "TensorrtExecutionProvider" in providers:
        print("  â³ TensorRTåˆå›æœ€é©åŒ–ä¸­... (æ•°åˆ†ã‹ã‹ã‚Šã¾ã™)")
        # å„ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã«å¯¾å¿œã™ã‚‹ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’è¨­å®š
        for provider in providers:
            if provider == "TensorrtExecutionProvider":
                provider_options.append(
                    {
                        "trt_max_workspace_size": "268435456",  # 256MB (æœ€å°åŒ–)
                        "trt_engine_cache_enable": "True",  # ã‚­ãƒ£ãƒƒã‚·ãƒ¥æœ‰åŠ¹
                        "trt_engine_cache_path": "./trt_cache",  # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‘ã‚¹
                    }
                )
            else:
                provider_options.append({})  # ä»–ã®ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ç”¨ã®ç©ºã‚ªãƒ—ã‚·ãƒ§ãƒ³
    elif "CUDAExecutionProvider" in providers:
        print("  âš¡ CUDAèµ·å‹•æœ€é©åŒ–ä¸­...")
        # å„ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã«å¯¾å¿œã™ã‚‹ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’è¨­å®š
        for provider in providers:
            if provider == "CUDAExecutionProvider":
                provider_options.append(
                    {
                        "device_id": 0,  # GPU ID
                        "arena_extend_strategy": "kNextPowerOfTwo",  # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–
                        "gpu_mem_limit": 2 * 1024 * 1024 * 1024,  # 2GBåˆ¶é™
                        "cudnn_conv_algo_search": "HEURISTIC",  # é«˜é€Ÿã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ é¸æŠ
                        "do_copy_in_default_stream": True,  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚¹ãƒˆãƒªãƒ¼ãƒ ä½¿ç”¨
                    }
                )
            else:
                provider_options.append({})  # ä»–ã®ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ç”¨ã®ç©ºã‚ªãƒ—ã‚·ãƒ§ãƒ³
    else:
        # CPUã®ã¿ã®å ´åˆ
        for provider in providers:
            provider_options.append({})

    session = ort.InferenceSession(str(model_path), sess_options, providers=providers, provider_options=provider_options)

    # å®Ÿéš›ã«ä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’è¡¨ç¤º
    active_provider = session.get_providers()[0]
    print(f"  ğŸš€ ä½¿ç”¨ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {active_provider}")
    if active_provider == "TensorrtExecutionProvider":
        print("  âš¡ TensorRTåŠ é€Ÿ: æœ€é«˜é€Ÿåº¦")
    elif active_provider == "CUDAExecutionProvider":
        print("  ğŸ”¥ CUDAåŠ é€Ÿ: é«˜é€Ÿ")
    else:
        print("  ğŸ–¥ï¸  CPUå®Ÿè¡Œ: æ¨™æº–")

    # å…¥åŠ›æƒ…å ±å–å¾—
    input_name = session.get_inputs()[0].name
    input_shape = session.get_inputs()[0].shape
    print(f"  å…¥åŠ›: {input_name}, Shape: {input_shape}")

    # å‡ºåŠ›æƒ…å ±å–å¾—
    output_names = [output.name for output in session.get_outputs()]
    print(f"  å‡ºåŠ›: {output_names}")

    # ã‚«ãƒ¡ãƒ©æ¥ç¶š
    cap = cv2.VideoCapture(camera_id)
    if not cap.isOpened():
        raise ValueError(f"USBã‚«ãƒ¡ãƒ©ã‚’é–‹ã‘ã¾ã›ã‚“: {camera_id}")

    # ã‚«ãƒ¡ãƒ©è¨­å®š
    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
    cap.set(cv2.CAP_PROP_FPS, fps_limit)

    # ã‚«ãƒ¡ãƒ©æƒ…å ±å–å¾—
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    actual_fps = cap.get(cv2.CAP_PROP_FPS)

    print(f"  ã‚«ãƒ¡ãƒ©æƒ…å ±: {width}x{height}, {actual_fps:.1f}fps")

    # éŒ²ç”»è¨­å®š
    output_writer = None
    if record_video:
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        output_path = Path(f"wani_camera_{timestamp}.mp4")
        fourcc = cv2.VideoWriter_fourcc(*"mp4v")
        output_writer = cv2.VideoWriter(str(output_path), fourcc, fps_limit, (width, height))
        print(f"ğŸ“¹ éŒ²ç”»é–‹å§‹: {output_path}")

    frame_count = 0
    total_detections = 0
    inference_times = []
    fps_control_time = 1.0 / fps_limit

    # ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³è¨­å®šï¼ˆå¸¸ã«ä½œæˆã—ã¦è¨­å®šã‚’èª­ã¿è¾¼ã¿ï¼‰
    calibration = WaniCalibration(center_x=width // 2, center_y=height // 2)

    def print_calibration_help():
        """ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ“ä½œæ–¹æ³•ã‚’è¡¨ç¤º"""
        print("\n" + "=" * 50)
        print("ğŸ¯ ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ¢ãƒ¼ãƒ‰æ“ä½œæ–¹æ³•")
        print("=" * 50)
        print("  åŸºæœ¬æ“ä½œ:")
        print("    'q': çµ‚äº†, 's': ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆ, '?': ã“ã®ãƒ˜ãƒ«ãƒ—ã‚’å†è¡¨ç¤º")
        print("  === ãƒ¯ãƒ‹ã‚¾ãƒ¼ãƒ³èª¿æ•´ ===")
        print("    'w/a/s/d': ä¸­å¿ƒä½ç½®èª¿æ•´")
        print("    'i/k': é–“éš”èª¿æ•´, 'j/l': æ¨ªå¹…èª¿æ•´, 'u/o': ç¸¦å¹…èª¿æ•´")
        print("  === ã‚«ãƒ¡ãƒ©èª¿æ•´ ===")
        print("    'y/h': ã‚¯ãƒ­ãƒƒãƒ—ä½ç½®ä¸Šä¸‹, 'g/f': ã‚¯ãƒ­ãƒƒãƒ—ä½ç½®å·¦å³")
        print("    '1/2': ã‚¯ãƒ­ãƒƒãƒ—å¹…èª¿æ•´, '3/4': ã‚¯ãƒ­ãƒƒãƒ—é«˜ã•èª¿æ•´")
        print("    'r/t': å›è»¢èª¿æ•´")
        print("=" * 50)

    if enable_calibration:
        print_calibration_help()
    else:
        print("âš¡ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ¨è«–é–‹å§‹...")
        print("  'q'ã‚­ãƒ¼ã§çµ‚äº†, 's'ã‚­ãƒ¼ã§ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆä¿å­˜")
        print("  ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ ã‚’è¡¨ç¤ºã—ã¾ã™")

    try:
        last_time = time.time()

        while True:
            # FPSåˆ¶å¾¡ï¼ˆã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ™‚ã¯ç„¡åŠ¹ï¼‰
            if not enable_calibration:
                current_time = time.time()
                elapsed = current_time - last_time
                if elapsed < fps_control_time:
                    time.sleep(fps_control_time - elapsed)

            ret, frame = cap.read()
            if not ret:
                print("âš ï¸ ãƒ•ãƒ¬ãƒ¼ãƒ ã®å–å¾—ã«å¤±æ•—")
                continue

            # ã‚«ãƒ¡ãƒ©å¤‰æ›ã‚’é©ç”¨ï¼ˆã‚¯ãƒ­ãƒƒãƒ—ãƒ»å›è»¢ï¼‰
            frame = calibration.apply_camera_transform(frame)

            frame_count += 1
            if not enable_calibration:
                last_time = time.time()

            # ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ™‚ã¯æ¨è«–ã‚’ã‚¹ã‚­ãƒƒãƒ—
            detections = []
            if not enable_calibration:
                # æ¨è«–å®Ÿè¡Œ
                start_time = time.time()
                detections, processed_frame = run_frame_inference(session, frame, input_name, output_names, conf_threshold)
                inference_time = (time.time() - start_time) * 1000
                inference_times.append(inference_time)
                total_detections += len(detections)

            # æ¤œå‡ºçµæœæç”»
            if enable_calibration:
                # ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ¢ãƒ¼ãƒ‰ï¼šæ ã®ã¿è¡¨ç¤º
                frame_with_detections = frame.copy()
                draw_calibration_zones(frame_with_detections, calibration)
            else:
                # é€šå¸¸ãƒ¢ãƒ¼ãƒ‰ï¼šæ¤œå‡ºçµæœ + ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ ã‚’è¡¨ç¤º
                if detections:
                    frame_with_detections = draw_detections_with_zones(frame, detections, calibration)
                else:
                    frame_with_detections = frame.copy()
                    draw_calibration_zones(frame_with_detections, calibration)

            # FPSã¨çµ±è¨ˆæƒ…å ±è¡¨ç¤º
            info_text = []
            if not enable_calibration and len(inference_times) > 0:
                avg_inference_time = np.mean(inference_times[-30:])
                current_fps = 1000 / avg_inference_time if avg_inference_time > 0 else 0
                info_text = [
                    f"FPS: {current_fps:.1f}",
                    f"Inference: {avg_inference_time:.1f}ms",
                    f"Detections: {len(detections)}",
                    f"Total: {total_detections}",
                ]

            # ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æƒ…å ±ã‚’è¿½åŠ 
            if enable_calibration:
                info_text.extend(calibration.get_status_text())
                info_text.insert(0, "CALIBRATION MODE")

            # æƒ…å ±ã‚’ç”»é¢ã«è¡¨ç¤º
            if info_text:
                y_offset = 30
                for i, text in enumerate(info_text):
                    cv2.putText(
                        frame_with_detections, text, (10, y_offset + i * 25), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2
                    )

            # ç”»é¢è¡¨ç¤º
            cv2.imshow("Wani Camera Detection", frame_with_detections)

            # ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã«ãƒ•ã‚©ãƒ¼ã‚«ã‚¹ã‚’è¨­å®šï¼ˆã‚­ãƒ¼å…¥åŠ›ã‚’ç¢ºå®Ÿã«å—ã‘å–ã‚‹ãŸã‚ï¼‰
            cv2.setWindowProperty("Wani Camera Detection", cv2.WND_PROP_TOPMOST, 1)
            cv2.setWindowProperty("Wani Camera Detection", cv2.WND_PROP_TOPMOST, 0)

            # éŒ²ç”»
            if output_writer is not None:
                output_writer.write(frame_with_detections)

            # ã‚­ãƒ¼å…¥åŠ›å‡¦ç†ï¼ˆå¿œç­”æ€§ã¨ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¬ãƒ¼ãƒˆã®ãƒãƒ©ãƒ³ã‚¹ï¼‰
            key = cv2.waitKey(30) & 0xFF
            if key == ord("q"):
                print("\nâ¹ï¸ ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒçµ‚äº†")
                break
            elif key == ord("s") and not enable_calibration:
                # ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆï¼ˆã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ™‚ã¯'s'ãŒä¸­å¿ƒYèª¿æ•´ã«ä½¿ç”¨ï¼‰
                timestamp = time.strftime("%Y%m%d_%H%M%S")
                screenshot_path = f"wani_screenshot_{timestamp}.jpg"
                cv2.imwrite(screenshot_path, frame_with_detections)
                print(f"  ğŸ“· ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆä¿å­˜: {screenshot_path}")
            elif enable_calibration:
                # ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç”¨ã‚­ãƒ¼æ“ä½œ
                if key == ord("w"):  # ä¸­å¿ƒYä¸Š
                    calibration.adjust_parameter("center_y", -1)
                    print(f"  ä¸­å¿ƒYä¸Šç§»å‹•: Y={calibration.center_y}")
                elif key == ord("s"):  # ä¸­å¿ƒYä¸‹
                    calibration.adjust_parameter("center_y", 1)
                    print(f"  ä¸­å¿ƒYä¸‹ç§»å‹•: Y={calibration.center_y}")
                elif key == ord("a"):  # ä¸­å¿ƒXå·¦
                    calibration.adjust_parameter("center_x", -1)
                    print(f"  ä¸­å¿ƒXå·¦ç§»å‹•: X={calibration.center_x}")
                elif key == ord("d"):  # ä¸­å¿ƒXå³
                    calibration.adjust_parameter("center_x", 1)
                    print(f"  ä¸­å¿ƒXå³ç§»å‹•: X={calibration.center_x}")
                elif key == ord("i"):  # é–“éš”æ‹¡å¤§
                    calibration.adjust_parameter("spacing", 1)
                    print(f"  é–“éš”æ‹¡å¤§: {calibration.spacing}")
                elif key == ord("k"):  # é–“éš”ç¸®å°
                    calibration.adjust_parameter("spacing", -1)
                    print(f"  é–“éš”ç¸®å°: {calibration.spacing}")
                elif key == ord("j"):  # æ¨ªå¹…ç¸®å°
                    calibration.adjust_parameter("width", -1)
                    print(f"  æ¨ªå¹…ç¸®å°: {calibration.width}")
                elif key == ord("l"):  # æ¨ªå¹…æ‹¡å¤§
                    calibration.adjust_parameter("width", 1)
                    print(f"  æ¨ªå¹…æ‹¡å¤§: {calibration.width}")
                elif key == ord("u"):  # ç¸¦å¹…ç¸®å°
                    calibration.adjust_parameter("height", -1)
                    print(f"  ç¸¦å¹…ç¸®å°: {calibration.height}")
                elif key == ord("o"):  # ç¸¦å¹…æ‹¡å¤§
                    calibration.adjust_parameter("height", 1)
                    print(f"  ç¸¦å¹…æ‹¡å¤§: {calibration.height}")
                # ã‚«ãƒ¡ãƒ©èª¿æ•´ã‚­ãƒ¼ï¼ˆé€šå¸¸ã‚­ãƒ¼ï¼‰
                elif key == ord("r"):  # åæ™‚è¨ˆå›ã‚Šå›è»¢
                    calibration.adjust_parameter("rotation", -1)
                    print(f"  åæ™‚è¨ˆå›ã‚Šå›è»¢: {calibration.rotation_angle}Â°")
                elif key == ord("t"):  # æ™‚è¨ˆå›ã‚Šå›è»¢
                    calibration.adjust_parameter("rotation", 1)
                    print(f"  æ™‚è¨ˆå›ã‚Šå›è»¢: {calibration.rotation_angle}Â°")
                elif key == ord("y"):  # ã‚¯ãƒ­ãƒƒãƒ—Yä½ç½®ä¸Š
                    calibration.adjust_parameter("crop_y", -1)
                    print(f"  ã‚¯ãƒ­ãƒƒãƒ—ä½ç½®ä¸Š: Y={calibration.crop_y}")
                elif key == ord("h"):  # ã‚¯ãƒ­ãƒƒãƒ—Yä½ç½®ä¸‹
                    calibration.adjust_parameter("crop_y", 1)
                    print(f"  ã‚¯ãƒ­ãƒƒãƒ—ä½ç½®ä¸‹: Y={calibration.crop_y}")
                elif key == ord("g"):  # ã‚¯ãƒ­ãƒƒãƒ—Xä½ç½®å·¦
                    calibration.adjust_parameter("crop_x", -1)
                    print(f"  ã‚¯ãƒ­ãƒƒãƒ—ä½ç½®å·¦: X={calibration.crop_x}")
                elif key == ord("f"):  # ã‚¯ãƒ­ãƒƒãƒ—Xä½ç½®å³
                    calibration.adjust_parameter("crop_x", 1)
                    print(f"  ã‚¯ãƒ­ãƒƒãƒ—ä½ç½®å³: X={calibration.crop_x}")
                elif key == ord("1"):  # ã‚¯ãƒ­ãƒƒãƒ—å¹…ç¸®å°
                    calibration.adjust_parameter("crop_width", -1)
                    print(f"  ã‚¯ãƒ­ãƒƒãƒ—å¹…ç¸®å°: {calibration.crop_width}")
                elif key == ord("2"):  # ã‚¯ãƒ­ãƒƒãƒ—å¹…æ‹¡å¤§
                    calibration.adjust_parameter("crop_width", 1)
                    print(f"  ã‚¯ãƒ­ãƒƒãƒ—å¹…æ‹¡å¤§: {calibration.crop_width}")
                elif key == ord("3"):  # ã‚¯ãƒ­ãƒƒãƒ—é«˜ã•ç¸®å°
                    calibration.adjust_parameter("crop_height", -1)
                    print(f"  ã‚¯ãƒ­ãƒƒãƒ—é«˜ã•ç¸®å°: {calibration.crop_height}")
                elif key == ord("4"):  # ã‚¯ãƒ­ãƒƒãƒ—é«˜ã•æ‹¡å¤§
                    calibration.adjust_parameter("crop_height", 1)
                    print(f"  ã‚¯ãƒ­ãƒƒãƒ—é«˜ã•æ‹¡å¤§: {calibration.crop_height}")
                # ãƒ˜ãƒ«ãƒ—è¡¨ç¤º
                elif key == ord("?"):  # ãƒ˜ãƒ«ãƒ—è¡¨ç¤º
                    print_calibration_help()
                # ãƒ‡ãƒãƒƒã‚°ç”¨ï¼šã©ã®ã‚­ãƒ¼ãŒæŠ¼ã•ã‚ŒãŸã‹ã‚’è¡¨ç¤º
                elif key != 255:  # 255ã¯ä½•ã‚‚ã‚­ãƒ¼ãŒæŠ¼ã•ã‚Œã¦ã„ãªã„çŠ¶æ…‹
                    if 32 <= key <= 126:  # å°åˆ·å¯èƒ½æ–‡å­—ã®å ´åˆ
                        print(f"  ã‚­ãƒ¼æŠ¼ä¸‹: '{chr(key)}' (ã‚³ãƒ¼ãƒ‰: {key})")
                    else:
                        print(f"  ã‚­ãƒ¼æŠ¼ä¸‹: ç‰¹æ®Šã‚­ãƒ¼ (ã‚³ãƒ¼ãƒ‰: {key})")

            # é€²æ—è¡¨ç¤ºï¼ˆã‚³ãƒ³ã‚½ãƒ¼ãƒ«ï¼‰
            if frame_count % 100 == 0:
                avg_inference_time = np.mean(inference_times[-100:])
                avg_detections_per_frame = total_detections / frame_count
                print(
                    f"  ãƒ•ãƒ¬ãƒ¼ãƒ : {frame_count}, å¹³å‡æ¨è«–: {avg_inference_time:.1f}ms, "
                    f"å¹³å‡æ¤œå‡º: {avg_detections_per_frame:.2f}"
                )

    finally:
        cap.release()
        if output_writer is not None:
            output_writer.release()
        cv2.destroyAllWindows()

    # çµ±è¨ˆæƒ…å ±è¡¨ç¤º
    if len(inference_times) > 0:
        avg_inference_time = np.mean(inference_times)
        avg_detections_per_frame = total_detections / frame_count if frame_count > 0 else 0

        print("\nâœ… ã‚«ãƒ¡ãƒ©æ¨è«–çµ‚äº†!")
        print(f"  å‡¦ç†ãƒ•ãƒ¬ãƒ¼ãƒ æ•°: {frame_count}")
        print(f"  ç·æ¤œå‡ºæ•°: {total_detections}")
        print(f"  å¹³å‡æ¤œå‡ºæ•°/ãƒ•ãƒ¬ãƒ¼ãƒ : {avg_detections_per_frame:.2f}")
        print(f"  å¹³å‡æ¨è«–æ™‚é–“: {avg_inference_time:.1f}ms")
        print(f"  å®ŸåŠ¹FPS: {1000 / avg_inference_time:.1f}fps")

    return total_detections


def main():
    parser = argparse.ArgumentParser(description="ONNXå‹•ç”»ãƒ»ã‚«ãƒ¡ãƒ©æ¨è«–")
    parser.add_argument("--model", type=str, default="models/wani_detector.onnx", help="ONNXãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹")
    parser.add_argument("--camera", type=int, help="ã‚«ãƒ¡ãƒ©IDï¼ˆé€šå¸¸0ï¼‰- USBã‚«ãƒ¡ãƒ©ä½¿ç”¨æ™‚")
    parser.add_argument("--conf", type=float, default=0.5, help="ä¿¡é ¼åº¦é–¾å€¤")
    parser.add_argument("--record", action="store_true", help="ã‚«ãƒ¡ãƒ©æ˜ åƒã‚’éŒ²ç”»ï¼ˆã‚«ãƒ¡ãƒ©ã®ã¿ï¼‰")
    parser.add_argument("--fps", type=int, default=30, help="ã‚«ãƒ¡ãƒ©FPSåˆ¶é™")
    parser.add_argument("--calibrate", action="store_true", help="5åŒ¹ãƒ¯ãƒ‹ä½ç½®ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ¢ãƒ¼ãƒ‰")
    parser.add_argument(
        "--provider",
        type=str,
        default="cpu",
        choices=["tensorrt", "cuda", "cpu"],
        help="å®Ÿè¡Œãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: cpu(CPUãƒ»ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ), cuda(CUDA), tensorrt(TensorRT)",
    )

    args = parser.parse_args()

    print("=" * 50)
    print("ğŸš€ ONNX Runtimeæ¨è«–")
    print("=" * 50)

    # ãƒ¢ãƒ‡ãƒ«å­˜åœ¨ç¢ºèª
    if not Path(args.model).exists():
        print(f"âŒ ãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {args.model}")
        return

    # å…¥åŠ›ãƒ¢ãƒ¼ãƒ‰åˆ¤å®š
    if args.camera is not None:
        # ã‚«ãƒ¡ãƒ©ãƒ¢ãƒ¼ãƒ‰
        print(f"ğŸ“· USBã‚«ãƒ¡ãƒ©ãƒ¢ãƒ¼ãƒ‰: Camera ID {args.camera}")
        run_camera_inference(
            args.model,
            camera_id=args.camera,
            conf_threshold=args.conf,
            record_video=args.record,
            fps_limit=args.fps,
            enable_calibration=args.calibrate,
            provider=args.provider,
        )
    # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ¢ãƒ¼ãƒ‰ã¯ç¾åœ¨ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã¾ã›ã‚“

    else:
        print("âŒ --camera ã‚’æŒ‡å®šã—ã¦ãã ã•ã„")
        print("\nä½¿ç”¨ä¾‹:")
        print("  ã‚«ãƒ¡ãƒ©: python wani_detector.py --camera 0 --record")
        print("  ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³: python wani_detector.py --camera 0 --calibrate")
        parser.print_help()


if __name__ == "__main__":
    main()
